{
  "program_info": {
    "name": "Huntr AI/ML Security Research",
    "url": "https://huntr.com/bounties",
    "focus": "AI/ML Security vulnerabilities in open source projects",
    "authorization": "Open source project security research",
    "last_updated": "2025-09-25T13:13:03.451828"
  },
  "authorized_scope": {
    "model_file_formats": [
      "TensorRT ($4,000)",
      "GGUF ($4,000)",
      "Joblib ($4,000)",
      "Keras Native ($4,000)",
      "ONNX ($4,000)",
      "SafeTensors ($4,000)",
      "TensorFlow Saved Model ($4,000)",
      "PyTorch models ($500-2000)",
      "Hugging Face models ($500-2000)"
    ],
    "ml_frameworks": [
      "PyTorch (Mobile)",
      "TensorFlow Lite (Mobile)",
      "Core ML (iOS)",
      "ML Kit (Android)",
      "ONNX Runtime Mobile",
      "Hugging Face Transformers",
      "Scikit-learn"
    ],
    "mobile_ml_testing": {
      "ios_targets": [
        "Core ML model security",
        "iOS ML inference vulnerabilities",
        "TensorFlow Lite iOS implementation",
        "PyTorch Mobile iOS security"
      ],
      "android_targets": [
        "TensorFlow Lite Android vulnerabilities",
        "ML Kit security issues",
        "ONNX Runtime Android implementation",
        "PyTorch Mobile Android security"
      ]
    }
  },
  "bounty_ranges": {
    "critical": "$4,000",
    "high": "$2,000-3,000",
    "medium": "$1,000-1,500",
    "low": "$500-800"
  },
  "testing_methodology": {
    "static_analysis": "ML model file format analysis",
    "dynamic_testing": "Runtime ML inference security testing",
    "mobile_specific": "Mobile ML framework security assessment",
    "evidence_required": "Proof-of-concept with model exploitation"
  }
}