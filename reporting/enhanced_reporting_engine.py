#!/usr/bin/env python3
"""
QuantumSentinel-Nexus v6.0 - Enhanced Reporting Engine
Comprehensive reporting system for bug bounty submissions and security assessments

Features:
- Platform-specific report templates
- Automated evidence collection and attachment
- CVSS scoring integration and risk assessment
- Executive summaries for different stakeholders
- Compliance reporting (OWASP, NIST, ISO 27001)
- Real-time report generation and delivery
"""

import asyncio
import json
import logging
import os
import base64
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import hashlib
import uuid
from jinja2 import Environment, FileSystemLoader, select_autoescape
from weasyprint import HTML, CSS
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from io import BytesIO

# Import security manager
try:
    from core.security.security_manager import SecurityManager, EncryptionManager
except ImportError:
    SecurityManager = None
    EncryptionManager = None

# Import vulnerability scanner and platform integration
try:
    from vulnerability_scanning.enhanced_scanner import ScanResult
    from bug_bounty_platforms.base_platform import Vulnerability, SubmissionResult
except ImportError:
    ScanResult = None
    Vulnerability = None
    SubmissionResult = None

@dataclass
class ReportConfig:
    """Report generation configuration"""
    template_type: str  # platform_submission, executive_summary, technical_detailed, compliance
    output_format: str  # pdf, html, json, markdown
    include_evidence: bool = True
    include_screenshots: bool = True
    include_executive_summary: bool = True
    include_technical_details: bool = True
    include_remediation: bool = True
    include_compliance_mapping: bool = False
    encrypt_sensitive_data: bool = True
    watermark: Optional[str] = None
    branding: Optional[Dict[str, str]] = None
    
    def __post_init__(self):
        if self.branding is None:
            self.branding = {
                'logo_path': '',
                'company_name': 'QuantumSentinel-Nexus',
                'report_footer': 'Generated by QuantumSentinel-Nexus Security Framework'
            }

@dataclass
class ReportMetadata:
    """Report metadata and tracking information"""
    report_id: str
    generated_at: datetime
    generated_by: str
    report_type: str
    target_scope: List[str]
    platform: Optional[str] = None
    program_id: Optional[str] = None
    encryption_enabled: bool = False
    file_hash: Optional[str] = None
    file_size: Optional[int] = None
    
    def __post_init__(self):
        if not self.report_id:
            self.report_id = str(uuid.uuid4())
        if not self.generated_at:
            self.generated_at = datetime.utcnow()

@dataclass
class EvidenceItem:
    """Evidence item for vulnerability reports"""
    evidence_type: str  # screenshot, request_response, log_file, code_snippet
    title: str
    description: str
    data: bytes
    content_type: str
    filename: str
    timestamp: datetime = None
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.utcnow()
        if self.metadata is None:
            self.metadata = {}

class EnhancedReportingEngine:
    """Advanced reporting engine for security assessments and bug bounty submissions"""

    def __init__(self, templates_dir: str = "templates", output_dir: str = "reports"):
        self.templates_dir = Path(templates_dir)
        self.output_dir = Path(output_dir)
        self.logger = logging.getLogger('QuantumSentinel.Reporting.Engine')

        # Security components
        self.security_manager = SecurityManager() if SecurityManager else None
        self.encryption_manager = EncryptionManager() if EncryptionManager else None

        # Template engine
        self.jinja_env = Environment(
            loader=FileSystemLoader(self.templates_dir),
            autoescape=select_autoescape(['html', 'xml'])
        )

        # Report storage and tracking
        self.generated_reports = {}
        self.report_templates = {}

        # Evidence storage
        self.evidence_storage = {}

        # Platform-specific configurations
        self.platform_configs = self._load_platform_configs()

        # Compliance frameworks
        self.compliance_mappings = self._load_compliance_mappings()

        # Report metrics
        self.reporting_metrics = {
            'total_reports_generated': 0,
            'platform_submissions': 0,
            'executive_reports': 0,
            'compliance_reports': 0,
            'total_evidence_items': 0,
            'encrypted_reports': 0
        }

        # Ensure output directory exists
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Initialize formatters for different output types
        self.formatters = {
            'html': HTMLFormatter(),
            'pdf': PDFFormatter(),
            'json': JSONFormatter(),
            'xml': XMLFormatter(),
            'docx': DOCXFormatter(),
            'excel': ExcelFormatter()
        }
    
    async def initialize(self) -> bool:
        """Initialize reporting engine"""
        try:
            # Load report templates
            await self._load_report_templates()
            
            # Initialize visualization settings
            self._setup_visualization_style()
            
            # Create default templates if they don't exist
            await self._create_default_templates()
            
            self.logger.info("✅ Enhanced reporting engine initialized")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to initialize reporting engine: {e}")
            return False
    
    def _load_platform_configs(self) -> Dict[str, Dict[str, Any]]:
        """Load platform-specific reporting configurations"""
        return {
            'hackerone': {
                'required_fields': ['title', 'vulnerability_information', 'impact', 'proof_of_concept'],
                'severity_mapping': {'critical': 'Critical', 'high': 'High', 'medium': 'Medium', 'low': 'Low'},
                'max_attachment_size': 25 * 1024 * 1024,  # 25MB
                'supported_formats': ['png', 'jpg', 'gif', 'txt', 'pdf'],
                'template_name': 'hackerone_submission.html'
            },
            'bugcrowd': {
                'required_fields': ['title', 'description', 'impact', 'proof_of_concept', 'remediation_advice'],
                'severity_mapping': {'critical': 'P1', 'high': 'P2', 'medium': 'P3', 'low': 'P4'},
                'max_attachment_size': 50 * 1024 * 1024,  # 50MB
                'supported_formats': ['png', 'jpg', 'gif', 'txt', 'pdf', 'mp4'],
                'template_name': 'bugcrowd_submission.html'
            },
            'intigriti': {
                'required_fields': ['title', 'description', 'impact', 'steps_to_reproduce'],
                'severity_mapping': {'critical': 'Critical', 'high': 'High', 'medium': 'Medium', 'low': 'Low'},
                'max_attachment_size': 20 * 1024 * 1024,  # 20MB
                'supported_formats': ['png', 'jpg', 'pdf', 'txt'],
                'template_name': 'intigriti_submission.html'
            },
            'google_vrp': {
                'required_fields': ['summary', 'product', 'version', 'attack_scenario', 'impact'],
                'severity_mapping': {'critical': 'Critical', 'high': 'High', 'medium': 'Medium', 'low': 'Low'},
                'max_attachment_size': 10 * 1024 * 1024,  # 10MB
                'supported_formats': ['png', 'jpg', 'txt'],
                'template_name': 'google_vrp_submission.html'
            }
        }
    
    def _load_compliance_mappings(self) -> Dict[str, Dict[str, Any]]:
        """Load compliance framework mappings"""
        return {
            'owasp_top_10_2023': {
                'A01_broken_access_control': ['authentication_bypass', 'privilege_escalation', 'idor'],
                'A02_cryptographic_failures': ['weak_encryption', 'insecure_storage', 'weak_hashing'],
                'A03_injection': ['sql_injection', 'command_injection', 'ldap_injection', 'xss'],
                'A04_insecure_design': ['business_logic', 'workflow_bypass', 'insufficient_validation'],
                'A05_security_misconfiguration': ['cloud_misconfiguration', 'default_credentials'],
                'A06_vulnerable_components': ['outdated_libraries', 'known_vulnerabilities'],
                'A07_identification_failures': ['weak_authentication', 'session_management'],
                'A08_integrity_failures': ['unsigned_updates', 'insecure_ci_cd'],
                'A09_logging_failures': ['insufficient_logging', 'log_injection'],
                'A10_ssrf': ['server_side_request_forgery', 'internal_service_access']
            },
            'nist_sp_800_115': {
                'planning': ['scope_definition', 'rules_of_engagement'],
                'discovery': ['reconnaissance', 'vulnerability_scanning'],
                'attack': ['exploitation', 'privilege_escalation'],
                'reporting': ['vulnerability_documentation', 'risk_assessment']
            },
            'iso_27001': {
                'A.12.6.1': ['vulnerability_management'],
                'A.14.2.1': ['secure_development'],
                'A.14.2.5': ['system_security_testing']
            }
        }
    
    def _setup_visualization_style(self):
        """Setup matplotlib and seaborn styling for reports"""
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        plt.rcParams['figure.figsize'] = (10, 6)
        plt.rcParams['font.size'] = 12
    
    async def _load_report_templates(self):
        """Load Jinja2 report templates"""
        template_files = {
            'platform_submission': 'platform_submission.html',
            'executive_summary': 'executive_summary.html',
            'technical_detailed': 'technical_detailed.html',
            'compliance_report': 'compliance_report.html'
        }
        
        for template_type, filename in template_files.items():
            try:
                template_path = self.templates_dir / filename
                if template_path.exists():
                    self.report_templates[template_type] = self.jinja_env.get_template(filename)
                else:
                    self.logger.warning(f"Template file not found: {filename}")
            except Exception as e:
                self.logger.error(f"Failed to load template {filename}: {e}")
    
    async def _create_default_templates(self):
        """Create default report templates if they don't exist"""
        self.templates_dir.mkdir(parents=True, exist_ok=True)
        
        # Default platform submission template
        platform_template = '''
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{{ vulnerability.title }} - Security Report</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; margin: 40px; }
        .header { border-bottom: 2px solid #333; padding-bottom: 20px; margin-bottom: 30px; }
        .severity-{{ vulnerability.severity.lower() }} { 
            padding: 5px 10px; 
            border-radius: 5px; 
            color: white;
            display: inline-block;
        }
        .severity-critical { background-color: #dc3545; }
        .severity-high { background-color: #fd7e14; }
        .severity-medium { background-color: #ffc107; color: black; }
        .severity-low { background-color: #28a745; }
        .section { margin: 30px 0; }
        .evidence { border: 1px solid #ddd; padding: 15px; margin: 10px 0; }
        .code { background-color: #f8f9fa; padding: 15px; border-left: 4px solid #007bff; }
        .footer { margin-top: 50px; padding-top: 20px; border-top: 1px solid #ddd; font-size: 0.9em; color: #666; }
    </style>
</head>
<body>
    <div class="header">
        <h1>{{ vulnerability.title }}</h1>
        <p><strong>Severity:</strong> <span class="severity-{{ vulnerability.severity.lower() }}">{{ vulnerability.severity.upper() }}</span></p>
        <p><strong>Discovered:</strong> {{ vulnerability.discovered_at.strftime('%Y-%m-%d %H:%M:%S UTC') }}</p>
        {% if vulnerability.cvss_score %}
        <p><strong>CVSS Score:</strong> {{ vulnerability.cvss_score }}/10.0</p>
        {% endif %}
    </div>
    
    <div class="section">
        <h2>Description</h2>
        <p>{{ vulnerability.description }}</p>
    </div>
    
    <div class="section">
        <h2>Proof of Concept</h2>
        <div class="code">
            <pre>{{ vulnerability.proof_of_concept }}</pre>
        </div>
    </div>
    
    <div class="section">
        <h2>Impact</h2>
        <p>{{ vulnerability.impact }}</p>
    </div>
    
    <div class="section">
        <h2>Remediation</h2>
        <p>{{ vulnerability.remediation }}</p>
    </div>
    
    {% if evidence_items %}
    <div class="section">
        <h2>Evidence</h2>
        {% for evidence in evidence_items %}
        <div class="evidence">
            <h3>{{ evidence.title }}</h3>
            <p>{{ evidence.description }}</p>
            <p><strong>Type:</strong> {{ evidence.evidence_type }}</p>
            <p><strong>Timestamp:</strong> {{ evidence.timestamp.strftime('%Y-%m-%d %H:%M:%S UTC') }}</p>
        </div>
        {% endfor %}
    </div>
    {% endif %}
    
    <div class="footer">
        <p>{{ config.branding.report_footer }}</p>
        <p>Report ID: {{ report_metadata.report_id }}</p>
        <p>Generated: {{ report_metadata.generated_at.strftime('%Y-%m-%d %H:%M:%S UTC') }}</p>
    </div>
</body>
</html>
        '''
        
        template_file = self.templates_dir / 'platform_submission.html'
        if not template_file.exists():
            with open(template_file, 'w') as f:
                f.write(platform_template)
            self.logger.info("Created default platform submission template")
    
    async def generate_vulnerability_report(self, vulnerability: Dict[str, Any], 
                                          evidence_items: List[EvidenceItem] = None,
                                          config: ReportConfig = None) -> Dict[str, Any]:
        """Generate comprehensive vulnerability report"""
        if not config:
            config = ReportConfig(template_type='platform_submission', output_format='pdf')
        
        if evidence_items is None:
            evidence_items = []
        
        # Generate report metadata
        report_metadata = ReportMetadata(
            report_id=str(uuid.uuid4()),
            generated_at=datetime.utcnow(),
            generated_by="QuantumSentinel-Nexus",
            report_type=config.template_type,
            target_scope=[vulnerability.get('affected_url', 'unknown')],
            encryption_enabled=config.encrypt_sensitive_data
        )
        
        try:
            # Generate visualizations if needed
            charts = await self._generate_report_charts(vulnerability, evidence_items)
            
            # Prepare template context
            template_context = {
                'vulnerability': vulnerability,
                'evidence_items': evidence_items,
                'report_metadata': report_metadata,
                'config': config,
                'charts': charts,
                'compliance_mappings': self._get_compliance_mappings(vulnerability.get('vulnerability_type', '')),
                'generated_at': datetime.utcnow(),
                'cvss_breakdown': self._calculate_cvss_breakdown(vulnerability)
            }
            
            # Generate report content
            report_content = await self._render_template(config.template_type, template_context)
            
            # Convert to requested format
            output_file = await self._convert_to_format(report_content, config, report_metadata)
            
            # Encrypt if requested
            if config.encrypt_sensitive_data and self.encryption_manager:
                output_file = await self._encrypt_report(output_file, report_metadata)
                self.reporting_metrics['encrypted_reports'] += 1
            
            # Store report metadata
            self.generated_reports[report_metadata.report_id] = {
                'metadata': report_metadata,
                'config': config,
                'file_path': output_file,
                'evidence_count': len(evidence_items)
            }
            
            # Update metrics
            self.reporting_metrics['total_reports_generated'] += 1
            if config.template_type == 'platform_submission':
                self.reporting_metrics['platform_submissions'] += 1
            elif config.template_type == 'executive_summary':
                self.reporting_metrics['executive_reports'] += 1
            elif config.template_type == 'compliance_report':
                self.reporting_metrics['compliance_reports'] += 1
            
            self.logger.info(f"✅ Generated {config.template_type} report: {report_metadata.report_id}")
            
            return {
                'success': True,
                'report_id': report_metadata.report_id,
                'file_path': str(output_file),
                'format': config.output_format,
                'encrypted': config.encrypt_sensitive_data,
                'metadata': asdict(report_metadata)
            }
            
        except Exception as e:
            self.logger.error(f"Failed to generate vulnerability report: {e}")
            return {
                'success': False,
                'error': str(e),
                'report_id': report_metadata.report_id
            }
    
    async def _generate_report_charts(self, vulnerability: Dict[str, Any], 
                                    evidence_items: List[EvidenceItem]) -> Dict[str, str]:
        """Generate charts and visualizations for the report"""
        charts = {}
        
        try:
            # CVSS score visualization
            if vulnerability.get('cvss_score'):
                cvss_chart = await self._create_cvss_chart(vulnerability)
                charts['cvss_visualization'] = cvss_chart
            
            # Severity distribution if multiple vulnerabilities
            # (This would be used for assessment reports with multiple findings)
            
            # Timeline chart for evidence
            if evidence_items:
                timeline_chart = await self._create_evidence_timeline(evidence_items)
                charts['evidence_timeline'] = timeline_chart
            
        except Exception as e:
            self.logger.warning(f"Failed to generate charts: {e}")
        
        return charts
    
    async def _create_cvss_chart(self, vulnerability: Dict[str, Any]) -> str:
        """Create CVSS score visualization"""
        fig, ax = plt.subplots(figsize=(8, 6))
        
        cvss_score = vulnerability.get('cvss_score', 0)
        severity = vulnerability.get('severity', 'unknown').lower()
        
        # Create a gauge-style chart
        categories = ['None', 'Low', 'Medium', 'High', 'Critical']
        scores = [0, 3.9, 6.9, 8.9, 10.0]
        colors = ['#28a745', '#ffc107', '#fd7e14', '#dc3545', '#6f42c1']
        
        # Determine current category
        current_category = 0
        for i, score in enumerate(scores[1:], 1):
            if cvss_score <= score:
                current_category = i
                break
        
        # Create horizontal bar chart
        y_pos = range(len(categories))
        ax.barh(y_pos, scores, color=colors, alpha=0.7)
        
        # Highlight current score
        ax.axvline(x=cvss_score, color='red', linestyle='--', linewidth=2, label=f'Current Score: {cvss_score}')
        
        ax.set_yticks(y_pos)
        ax.set_yticklabels(categories)
        ax.set_xlabel('CVSS Score')
        ax.set_title(f'CVSS Score Visualization - {severity.capitalize()} Severity')
        ax.legend()
        
        # Save to base64 string
        img_buffer = BytesIO()
        plt.savefig(img_buffer, format='png', dpi=150, bbox_inches='tight')
        img_buffer.seek(0)
        img_base64 = base64.b64encode(img_buffer.getvalue()).decode()
        plt.close()
        
        return f"data:image/png;base64,{img_base64}"
    
    async def _create_evidence_timeline(self, evidence_items: List[EvidenceItem]) -> str:
        """Create timeline visualization for evidence items"""
        if not evidence_items:
            return ""
        
        fig, ax = plt.subplots(figsize=(12, 6))
        
        # Extract timestamps and titles
        timestamps = [item.timestamp for item in evidence_items]
        titles = [item.title[:30] + '...' if len(item.title) > 30 else item.title for item in evidence_items]
        
        # Create timeline plot
        ax.scatter(timestamps, range(len(timestamps)), s=100, alpha=0.7)
        
        for i, (timestamp, title) in enumerate(zip(timestamps, titles)):
            ax.annotate(title, (timestamp, i), xytext=(10, 0), textcoords='offset points', 
                       fontsize=9, ha='left')
        
        ax.set_yticks(range(len(timestamps)))
        ax.set_yticklabels([f"Evidence {i+1}" for i in range(len(timestamps))])
        ax.set_xlabel('Timestamp')
        ax.set_title('Evidence Collection Timeline')
        
        # Format x-axis
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        # Save to base64 string
        img_buffer = BytesIO()
        plt.savefig(img_buffer, format='png', dpi=150, bbox_inches='tight')
        img_buffer.seek(0)
        img_base64 = base64.b64encode(img_buffer.getvalue()).decode()
        plt.close()
        
        return f"data:image/png;base64,{img_base64}"
    
    def _get_compliance_mappings(self, vulnerability_type: str) -> Dict[str, List[str]]:
        """Get compliance framework mappings for vulnerability type"""
        mappings = {}
        
        for framework, categories in self.compliance_mappings.items():
            framework_mappings = []
            for category, vuln_types in categories.items():
                if vulnerability_type.lower() in vuln_types:
                    framework_mappings.append(category)
            if framework_mappings:
                mappings[framework] = framework_mappings
        
        return mappings
    
    def _calculate_cvss_breakdown(self, vulnerability: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate CVSS score breakdown"""
        # Simplified CVSS breakdown
        cvss_score = vulnerability.get('cvss_score', 0)
        
        return {
            'base_score': cvss_score,
            'impact_subscore': min(10.0, cvss_score * 0.6),
            'exploitability_subscore': min(10.0, cvss_score * 0.4),
            'severity_rating': self._get_severity_from_cvss(cvss_score)
        }
    
    def _get_severity_from_cvss(self, cvss_score: float) -> str:
        """Get severity rating from CVSS score"""
        if cvss_score >= 9.0:
            return 'Critical'
        elif cvss_score >= 7.0:
            return 'High'
        elif cvss_score >= 4.0:
            return 'Medium'
        elif cvss_score > 0.0:
            return 'Low'
        else:
            return 'None'
    
    async def _render_template(self, template_type: str, context: Dict[str, Any]) -> str:
        """Render Jinja2 template with context"""
        if template_type not in self.report_templates:
            raise ValueError(f"Template type '{template_type}' not found")
        
        template = self.report_templates[template_type]
        return template.render(**context)
    
    async def _convert_to_format(self, html_content: str, config: ReportConfig, 
                               metadata: ReportMetadata) -> Path:
        """Convert HTML content to requested format"""
        output_filename = f"{metadata.report_id}.{config.output_format}"
        output_path = self.output_dir / output_filename
        
        if config.output_format == 'html':
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
        
        elif config.output_format == 'pdf':
            # Convert HTML to PDF using weasyprint
            html_doc = HTML(string=html_content, base_url=str(self.templates_dir))
            
            # Apply custom CSS if available
            css_path = self.templates_dir / 'report_styles.css'
            if css_path.exists():
                css = CSS(filename=str(css_path))
                html_doc.write_pdf(str(output_path), stylesheets=[css])
            else:
                html_doc.write_pdf(str(output_path))
        
        elif config.output_format == 'json':
            # Export as structured JSON
            json_data = {
                'metadata': asdict(metadata),
                'config': asdict(config),
                'html_content': html_content
            }
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, indent=2, default=str)
        
        else:
            raise ValueError(f"Unsupported output format: {config.output_format}")
        
        # Calculate file hash and size
        with open(output_path, 'rb') as f:
            file_content = f.read()
            metadata.file_hash = hashlib.sha256(file_content).hexdigest()
            metadata.file_size = len(file_content)
        
        return output_path
    
    async def _encrypt_report(self, file_path: Path, metadata: ReportMetadata) -> Path:
        """Encrypt report file"""
        if not self.encryption_manager:
            self.logger.warning("Encryption requested but encryption manager not available")
            return file_path
        
        try:
            # Read file content
            with open(file_path, 'rb') as f:
                file_content = f.read()
            
            # Encrypt content
            encrypted_content = self.encryption_manager.encrypt(base64.b64encode(file_content).decode())
            
            # Save encrypted file
            encrypted_path = file_path.with_suffix(f"{file_path.suffix}.encrypted")
            with open(encrypted_path, 'w') as f:
                f.write(encrypted_content)
            
            # Remove original file
            file_path.unlink()
            
            metadata.encryption_enabled = True
            return encrypted_path
            
        except Exception as e:
            self.logger.error(f"Failed to encrypt report: {e}")
            return file_path
    
    async def generate_platform_submission_report(self, vulnerability: Dict[str, Any], 
                                                 platform: str, 
                                                 evidence_items: List[EvidenceItem] = None) -> Dict[str, Any]:
        """Generate platform-specific submission report"""
        if platform.lower() not in self.platform_configs:
            raise ValueError(f"Unsupported platform: {platform}")
        
        platform_config = self.platform_configs[platform.lower()]
        
        # Validate required fields
        missing_fields = []
        for field in platform_config['required_fields']:
            if field not in vulnerability or not vulnerability[field]:
                missing_fields.append(field)
        
        if missing_fields:
            return {
                'success': False,
                'error': f"Missing required fields for {platform}: {', '.join(missing_fields)}"
            }
        
        # Create platform-specific report config
        config = ReportConfig(
            template_type='platform_submission',
            output_format='pdf',
            include_evidence=True,
            include_screenshots=True
        )
        
        # Generate report
        result = await self.generate_vulnerability_report(vulnerability, evidence_items, config)
        
        if result['success']:
            result['platform'] = platform
            result['platform_requirements'] = platform_config
        
        return result
    
    async def generate_executive_summary(self, assessment_results: List[Dict[str, Any]], 
                                       assessment_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Generate executive summary report"""
        # Analyze assessment results
        summary_stats = self._analyze_assessment_results(assessment_results)
        
        # Create executive summary context
        template_context = {
            'assessment_metadata': assessment_metadata,
            'summary_stats': summary_stats,
            'findings': assessment_results,
            'recommendations': self._generate_executive_recommendations(summary_stats),
            'risk_assessment': self._calculate_overall_risk(assessment_results),
            'generated_at': datetime.utcnow()
        }
        
        config = ReportConfig(
            template_type='executive_summary',
            output_format='pdf',
            include_executive_summary=True,
            include_technical_details=False
        )
        
        # Generate report metadata
        report_metadata = ReportMetadata(
            report_id=str(uuid.uuid4()),
            generated_at=datetime.utcnow(),
            generated_by="QuantumSentinel-Nexus",
            report_type='executive_summary',
            target_scope=assessment_metadata.get('targets', [])
        )
        
        try:
            # Render template
            html_content = await self._render_template('executive_summary', template_context)
            
            # Convert to PDF
            output_file = await self._convert_to_format(html_content, config, report_metadata)
            
            return {
                'success': True,
                'report_id': report_metadata.report_id,
                'file_path': str(output_file),
                'summary_stats': summary_stats
            }
            
        except Exception as e:
            self.logger.error(f"Failed to generate executive summary: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _analyze_assessment_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze assessment results for summary statistics"""
        if not results:
            return {
                'total_findings': 0,
                'severity_distribution': {},
                'vulnerability_types': {},
                'risk_score': 0.0
            }
        
        severity_counts = {}
        vuln_type_counts = {}
        total_cvss = 0.0
        cvss_count = 0
        
        for result in results:
            # Count severities
            severity = result.get('severity', 'unknown').lower()
            severity_counts[severity] = severity_counts.get(severity, 0) + 1
            
            # Count vulnerability types
            vuln_type = result.get('vulnerability_type', 'unknown')
            vuln_type_counts[vuln_type] = vuln_type_counts.get(vuln_type, 0) + 1
            
            # Calculate average CVSS
            if result.get('cvss_score'):
                total_cvss += result['cvss_score']
                cvss_count += 1
        
        avg_cvss = total_cvss / cvss_count if cvss_count > 0 else 0.0
        
        return {
            'total_findings': len(results),
            'severity_distribution': severity_counts,
            'vulnerability_types': vuln_type_counts,
            'average_cvss_score': round(avg_cvss, 1),
            'risk_score': self._calculate_risk_score(severity_counts, avg_cvss)
        }
    
    def _calculate_risk_score(self, severity_counts: Dict[str, int], avg_cvss: float) -> float:
        """Calculate overall risk score (0-10 scale)"""
        severity_weights = {
            'critical': 10,
            'high': 7,
            'medium': 4,
            'low': 1
        }
        
        weighted_score = 0
        total_findings = sum(severity_counts.values())
        
        if total_findings == 0:
            return 0.0
        
        for severity, count in severity_counts.items():
            weight = severity_weights.get(severity, 0)
            weighted_score += weight * count
        
        # Normalize to 0-10 scale and factor in CVSS
        base_score = (weighted_score / total_findings)
        cvss_factor = avg_cvss / 10.0
        
        final_score = (base_score + cvss_factor * 10) / 2
        return round(min(10.0, final_score), 1)
    
    def _generate_executive_recommendations(self, summary_stats: Dict[str, Any]) -> List[str]:
        """Generate executive-level recommendations"""
        recommendations = []
        
        risk_score = summary_stats.get('risk_score', 0)
        severity_dist = summary_stats.get('severity_distribution', {})
        
        if risk_score >= 8.0:
            recommendations.append("Immediate action required: Critical security vulnerabilities identified")
        elif risk_score >= 6.0:
            recommendations.append("High priority: Significant security issues require prompt attention")
        elif risk_score >= 4.0:
            recommendations.append("Medium priority: Address identified vulnerabilities in next maintenance cycle")
        else:
            recommendations.append("Low risk: Continue monitoring and maintain current security posture")
        
        if severity_dist.get('critical', 0) > 0:
            recommendations.append("Deploy emergency patches for critical vulnerabilities")
        
        if severity_dist.get('high', 0) > 3:
            recommendations.append("Implement comprehensive security review process")
        
        recommendations.append("Regular security assessments and continuous monitoring recommended")
        
        return recommendations
    
    def _calculate_overall_risk(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Calculate overall risk assessment"""
        if not results:
            return {'level': 'Low', 'score': 0.0, 'factors': []}
        
        risk_factors = []
        critical_count = len([r for r in results if r.get('severity', '').lower() == 'critical'])
        high_count = len([r for r in results if r.get('severity', '').lower() == 'high'])
        
        if critical_count > 0:
            risk_factors.append(f"{critical_count} critical vulnerabilities")
        if high_count > 0:
            risk_factors.append(f"{high_count} high severity vulnerabilities")
        
        # Calculate risk level
        if critical_count > 0:
            risk_level = 'Critical'
            risk_score = 9.0 + min(1.0, critical_count * 0.2)
        elif high_count > 2:
            risk_level = 'High'
            risk_score = 7.0 + min(2.0, high_count * 0.3)
        elif high_count > 0:
            risk_level = 'Medium'
            risk_score = 5.0 + min(2.0, high_count * 0.5)
        else:
            risk_level = 'Low'
            risk_score = max(1.0, len(results) * 0.5)
        
        return {
            'level': risk_level,
            'score': min(10.0, risk_score),
            'factors': risk_factors
        }
    
    async def add_evidence_item(self, evidence: EvidenceItem) -> str:
        """Add evidence item to storage"""
        evidence_id = str(uuid.uuid4())
        self.evidence_storage[evidence_id] = evidence
        self.reporting_metrics['total_evidence_items'] += 1
        
        self.logger.info(f"Added evidence item: {evidence.title} ({evidence.evidence_type})")
        return evidence_id
    
    async def get_report_metrics(self) -> Dict[str, Any]:
        """Get comprehensive reporting metrics"""
        return {
            'reporting_metrics': self.reporting_metrics.copy(),
            'generated_reports_count': len(self.generated_reports),
            'evidence_items_count': len(self.evidence_storage),
            'supported_platforms': list(self.platform_configs.keys()),
            'compliance_frameworks': list(self.compliance_mappings.keys()),
            'template_types': list(self.report_templates.keys())
        }
    
    async def generate_html_report(self, report_data: Dict[str, Any]) -> str:
        """Generate comprehensive HTML report"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_path = f"./reports/quantum_sentinel_report_{timestamp}.html"

            # Use HTML formatter to generate report
            html_content = await self.formatters['html'].format_report(
                report_data,
                template_name="comprehensive_report"
            )

            Path("./reports").mkdir(exist_ok=True)

            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(html_content)

            return report_path

        except Exception as e:
            self.logger.error(f"Error generating HTML report: {e}")
            raise

    async def generate_pdf_report(self, report_data: Dict[str, Any]) -> str:
        """Generate comprehensive PDF report"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_path = f"./reports/quantum_sentinel_report_{timestamp}.pdf"

            # Use PDF formatter to generate report
            await self.formatters['pdf'].format_report(
                report_data,
                output_path=report_path,
                template_name="comprehensive_report"
            )

            return report_path

        except Exception as e:
            self.logger.error(f"Error generating PDF report: {e}")
            raise

    async def cleanup(self):
        """Clean up reporting engine resources"""
        # Clear caches
        self.generated_reports.clear()
        self.evidence_storage.clear()

        self.logger.info("✅ Enhanced reporting engine cleaned up")

# Example usage
if __name__ == "__main__":
    async def test_reporting_engine():
        """Test enhanced reporting engine"""
        engine = EnhancedReportingEngine()
        
        if await engine.initialize():
            print("✅ Reporting engine initialized")
            
            # Sample vulnerability data
            vulnerability = {
                'title': 'SQL Injection in Login Form',
                'vulnerability_type': 'sql_injection',
                'severity': 'high',
                'cvss_score': 8.1,
                'cvss_vector': 'CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:N',
                'affected_url': 'https://example.com/login',
                'description': 'SQL injection vulnerability in the login form allows unauthorized database access.',
                'proof_of_concept': "POST /login\nContent-Type: application/x-www-form-urlencoded\n\nusername=admin'--&password=test",
                'impact': 'Unauthorized access to user data and potential database compromise.',
                'remediation': 'Use parameterized queries and input validation.',
                'discovered_at': datetime.utcnow()
            }
            
            # Generate vulnerability report
            result = await engine.generate_vulnerability_report(vulnerability)
            if result['success']:
                print(f"Generated report: {result['report_id']}")
                print(f"File: {result['file_path']}")
            else:
                print(f"Report generation failed: {result['error']}")
            
            # Get metrics
            metrics = await engine.get_reporting_metrics()
            print(f"Reporting metrics: {json.dumps(metrics, indent=2)}")
            
            await engine.cleanup()
        else:
            print("❌ Failed to initialize reporting engine")
    
    # Run test
    asyncio.run(test_reporting_engine())
